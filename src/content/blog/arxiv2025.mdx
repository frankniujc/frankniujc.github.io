---
author: Jingcheng Niu
date: 2025-07-20
title: "Shape Happens: Automatic Feature Manifold Discovery in LLMs"
slug: arxiv2025
featured: false
draft: false
venue: Under Review
paper: /research/arxiv2025/shape_happens.pdf
paper_linkname: arXiv
code: https://github.com/UKPLab/arxiv2025-shape-happens

authors:
  - Federico Tiblias
  - Irina Bigoulaeva
  - Jingcheng Niu
  - Simone Balloccu
  - Iryna Gurevych
tags:
  - interpretability
  - ICL
  - induction head
---

## Links

- [Paper Draft Link](/research/arxiv2025/shape_happens.pdf).
- [GitHub Repository](https://github.com/UKPLab/arxiv2025-shape-happens).

**Abstract**: The linear representation hypothesis states that language models (LMs) encode concepts as directions in their latent space, forming organized, multidimensional manifolds. Prior efforts focus on discovering specific geometries for specific features, and thus lack generalization. We introduce Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to automatically discover feature manifolds. We apply SMDS to temporal reasoning as a case study, finding that different features form various geometric structures such as circles, lines, and clusters. SMDS reveals many insights on these structures: they consistently reflect the properties of the concepts they represent; are stable across model families and sizes; actively support reasoning in models; and dynamically reshape in response to context changes. Together, our findings shed light on the functional role of feature manifolds, supporting a model of entity-based reasoning in which LMs encode and transform structured representations.